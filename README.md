# Peval Testsets

[**Site**](https://peval.io) / [**ùïè**](https://x.com/fiveoutofnine)

Peval is an AI eval platform that assesses models using optimized prompts across various tasks and scenarios.
There are [ongoing competitions](https://peval.io) anyone can compete in by writing prompts to be evaluated.
As models improve and new techniques emerge, the prompts get more effective, which yields a benchmark that pushes models to their limits.

Currently, there is 1 type of competition:

1. **Competitions**: Prompts tested against a Q&amp;A-style test set.

## Competitions

Each Peval Competition has a **public** testset to score submissions while it's ongoing and a **private** testset to score submissions after end.
The following is a complete list of both testsets for each completed competition and the scripts used to generate them:

| Name                   | `questions.csv` | Peval                                                        |
| ---------------------- | --------------- | ------------------------------------------------------------ |
| Structured Grid Output |                 | [Link‚Üó](https://peval.io/competition/structured-grid-output) |
| LLM Multiplication     |                 | [Link‚Üó](https://peval.io/competition/llm-multiplication)     |

## Contribution

If you have any questions, ideas, issues, or requests, DM [@fiveoutofnine](https://x.com/fiveoutofnine).
